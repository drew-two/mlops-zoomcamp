# 5.1 - Monitoring for ML-based Services
- In production, models generate value and failures cost something

- ML models degrade and need monitoring
- If something is wrong, it is because the data is wrong
	- Data processing issues, or data loss at the source (sensors broken)
	- The environment of the model can change too
		- Input feature distribution or target function have changed
		- These are *data drift* and *concept drift*
	- At some point, model accuracy decreases and we need to find this moment to mitigate it
		- Ex: retrain model, rebuild it, fall back to some robust system

## How is machine learning monitoring different?
- We _can_ apply traditional monitoring but there are some more things to take care of
	- Data health
	- Model health

The main things to look at:
1. Service health
	- Monitoring schema and metrics will depend on how you deployed your model
	- Batch function learning models really depend on high load, real-time models
	- Need to verify models are actually working
2. Model performance
	- Exact metrics depend on the problem statement
	- Quality metrics for classification, regression, recommender systems etc. It depends
	- This is the feedback, with immediate feedback you can find quality metrics immediately
		- If you don't, like with forecast service, like user churn, you make prediction in a month maybe
		- Thus you have to wait. And you may have to apply same models again, and may multiply issues
3. Data quality and integrity
4. Data and concept drift

Extra things to monitor:
5. Performance by segment
6. Model bias/fairness
7. Outliers
8. Explainability

Depends on the sensitivity of your case and the risks you're willing to take
Ex: If you have a sensitive problem statement, you may want to monitor certain segments
	- New users, or new users from specific region. So you want metrics just from there
Ex: If the problem is medical or social:
	- May want model bias/fairness
Ex: If each individual euro costs you a lot:
	- May want outlier detection. If you catch them before you apply the model, you can send them somewhere else
		- Rule based system or manual review
Ex: Need to show user why things were done
	- May want to offer explainability
	- Especially if you retrain regularly and explanations may change

## Batch vs Online Serving Models
- How to monitor?
	- Generally if you have a live serving model, you want combined modelling, and with batch models you want batch monitoring
	- Most production models are in batch mode
		- So your pipeline is ran with workflow management with Prefect or Airflow
			- So you would keep your monitoring batch as well
		- Keep some calculation after each step of your pipeline and run some checks to ensure data and model behave as expected
		- Then log this data to database and aggregation layer
			- Make report with Tableau/Looker etc
	- So in real-time you want real-time metrics with frequent polling
		- So Prometheus and Grafana are a good combination
		- Sometimes you may want to keep monitoring batch, for metrics like data drift
			- No need to re-estimate after each row. Maybe after a batch
		- So you may still implement batch monitoring for a real-time model

## Practice
- Going to monitor existing ride prediction service
	- Takes some JSON input and return JSON predictions
![](images/5.1-diagram.png)
- Will make script to simulate production service
- Will store JSON inputs/predictions in MongoDB
	- Because our data is JSON format and MongoDB is NoSQL
	- Will need to create host, make database, create connection and push from there
		- Has Python library
- Will send same JSON inputs/predictions to monitoring service
	- This will send to Prometheus DB where Grafana will read from
	- Monitoring service will do calculations and expose HTTP endpoints for Prometheus 
	- Prometheus is convenient for online monitoring
- Prometheus optimized for real-time time-series data. 
- Prometheus and Grafana have strong integration
	- Prometheus also has some visualization tools but Grafana's are more powerful
- We can use Prefect to pull from MongoDB for batch monitoring
	- Take JSON inputs/preds/targets
	- Make Prefect flow to create model profiles (a lot of metrics)
	- Then Evidently will calculate metrics and create report
		- Will also send profiles back to MongoDB

- There are a lot of services, so we will containerize and use Dockerfiles 

## Making Scripts
- Go back to 04-deployment/web-service
	- Open predict.py. Note that it provides most of what we need
	- Will add two functions to simulate production
		- Will send a lot of data and log it somewhere
		- Will send data to monitoring service
- Back to 05-monitoring
- We will be using evidently_service/
- Go to prediction_service
	- Create app.py, import os, pickle, requests
	- Import pymongo, Flask
	- Create some constants for model, database, evidently address
	- Load model with pickle
	- Create flask app, connect to client at DB 'prediction_service' and get collection
	- Create function predict() at app.route /predict with method POST
		- Create feature PU_DO, perform dv.transform on record
		- Predict on new records
		- Make into result dictionary and convert y_pred to float
		- Save to db
		- Send to evidently service
		- Return
	- Create function save_to_db(record, prediction):
		- Make deep copy of record
		- Set rec['prediction'] as prediction
		- Insert that one record to collection
	- Same thing with send_to_evidently_service(record, prediction):
		- Do the same except post to EVIDENTLY_SERVICE_ADDRESS/iterate/taxi

## Running
- Bring up docker compose in 05-monitoring/
- Run 05-monitoring/test.py

- Back to architecture diagram
	- Our prediction service is up, connects to MongoDB
	- Sends same data to MongoDB and Monitoring Service (Prometheus) 


