# More experiment tracking with MLflow

We will see:
	- Adding parameter tuning to the notebook
	- Show it in MLflow
	- Select best parameters
	- Autolog
Needs code from 2.2 (mlops-zoompcamp/02-experiment-tracking/duration-prediction.ipynb)

Recalling we were adjusting this Lasso model's alpha.

New example with XGBoost model and try to optimize with hyperopt.
1. Export xgboost
2. Export fmin from hyperopt for optimization
	a. hyperopt is "a Python library for serial and parallel optimization over awkward search spaces"
	b. Algorithm controlled by tpe. hp for search space. STATUS_OK to tell hyperopt that the trial has run sucessfully, Trials records trials info
3. Need to create 'xgb.Dmatrix' which is a data type for the input data
	a. Create one for training and validation
4. Can create 'search space' variable - dict with params and possible values.
5. Define function for running training that takes params
	a. Can be logged to mlflow
	b. Also passed to xgboost model to generate 'booster' object ('boosted' model)
	c. Pass x_train and x_val dtypes
	c. xgboost will try to improve via validation set
6. Once trained, we will make prediction with validation for validation score with rmse
7. After, return loss score and status (STATUS_OK)

## Search space:
Can define variables with hyperopt methods to return values
	-hp.quniform returns real number (float) from round(uniform(low,high) / q) * q- converted to int
	-hp.loguniform returns value with logarithm constraint [exp(low), exp(high)]
Set these for different parameters like max tree depth and learning rate.
	- Objective set to reg:linear (linear regression), can be different models

Run fmin to run all this. Takes 
	- objective function
	- search space
	- the search algorithm
	- max number of evaluations
	- set trials to Trials() (records all the trial information)

## Back to MLflow
Look at our recent experiments. Each evaluation of fmin is one experiment for MLflow
	- Can see all the hyperopt parameters used, model used, rmse

Can filter by tags like tags.<tag_name> = <tag_value>

Can select all experiments and compare.
	- First you will see parallel coordinates plot - easy to observe all hyperparameters
		- Adding optimizing metric, you can see all the params used for each rmse value as a line.

Might be interested in seeing which parameters give the best results
	- Clicking on the metric bar highlights the parameters that lead to a certain section of scores
	- Can see which parameters correspond to better values for the score as the lines are highlighted 

Can check scatter plot of two values (e.g. parameter, metric)
	- Can see min_child_weight correspond to lower values of rmse

Can add a third hyperparameter with **contour plot**
	- Shows third parameter with chart colour.
	- Can zoom in on chart too

## Selecting best model
- Can sort results by your score metric on main experiments page
	- Get access to parameters of that metric
	- Training time might also be important
	- Max_depth (model size) that's lower may be important for size - will run faster.

Copies best parameters from MLflow and copies xgb booster object
	- Want to save model and log to mlflow
		- One way is to add with mlflow.start_run()
		- One other way is with MLflow autolog
			- If using some popular libraries (includes TF/Keras, Pytorch, SKlearn, xgb)
			- Just run mlflow.xgboost.autolog() 
				- Can see that more parameters are now automatically logged to mlflow
				- Can see <best_iteration, stopped_iteration, validation> x rmse graph over time
			- Saved conda env, requirements file, format of model in xgboost or just python

