# 2.1 - Experiment tracking intro

## Important concepts
- ML experiment: the process of building of an ML model
- Experiment run: each trial in an ML experiment
- Run artifact: any file associated with an ML run
- Experiment metadata

## What's experiment tracking?
The process of keeping track of all the **relevant information** from an **ML experiment**, which includes:
- Source code
- Environment
- Data
- Model
- Hyperparameters
- Metrics
- ...

In general, difficult to define set of information entities you want to include in your experiments, so we have standard metrics.
E.g source code, version, environment, whatever you need to keep track of

## Why is experiment tracking so important?
In general, because of these 3 reasons:
- Reproducibility - we are scientists
- Organization - needed for proper collaboration or even to document for yourself in the future
- Optimization (of your ML model) - need to keep track of information for when you go back to optimize

## Tracking experiments in spreadsheets
Very basic - why is it not enough?
- Error prone
- No standard format - can't share easily, another data scientist would not be able to improve easily
- Visibility & collaboration - bad to share in an organization

Want tools to make more efficient use of our time

## MLflow
Official documentation: https://mlflow.org/
Definition: "An open source platform for the machine learning lifecycle"
	- Refers to whole process of building and maintain ML model
	- Not the whole platform

In practice, it's just a Python package that can be installed with pip, and it contains four main modules:
- Tracking
- Models
- Model Registry
- Projects - out of scope for this course

## Tracking experiments with MLflow
The MLflow Tracking module allows you to organize your experiments into runs (each trial), and to keep track of:
- Parameters (hyperparameters, or anything else - e.g. dataset path, preprocessing)
- Metrics (val or test sets, any metrics like accuracy, F1)
- Metadata (Tags for searching past runs - e.g. name of developer, type of algorithm etc)
- Artifacts (Any file, dataset (might be too big), visualization images etc)
- Models (maybe not for hyperparameter tuning)

Along with this information, MLflow automatically logs extra information about the run:
- Source code
- Version of the code (git commit)
- Start and end time
- Author

## MLflow Example
- Installed as python package, available in command line
	- Can upload and download artifacts, serve models to Azure or sagemaker, run from a URI
	- Can run 'mlflow ui' to launch webpage gui or 'mlflow server' to launch server
		- Would have to forward port 5000 from cloud to local machine to access web page
	- Have to provide directory to store models, checkpoints, pickeled data etc
		- Can be local folder, S3 bucket etc
	- Two tabs in UI, Experiments and Models
		- Experiments tab solely keeps track of experiments
		- Model Registry requires backend DB (postgresql, mysql, sqlite or mssql, we'll get to this later)

