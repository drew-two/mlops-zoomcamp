# 4.4 - Machine Learning for Streaming

* Scenario
* Creating the role 
* Create a Lambda function, test it
* Create a Kinesis stream
	- Event stream like Kafka
* Connect the function to the stream
* Send the records 

- All AWS. Optional video (for homework)

- Recap: Wrote model to model registry
	- Called model in web application
	- Called it from another script via REST
	- Redid this but with a pipeline written to model registry

- Now: Imagine two models:
	- One as web-service, gives okay estimate
	- One with better performance, to deploy
		- (It's actually worse, but pretend it's better)

## Create Kinesis stream
https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html
- Lambdas: Run code without thinking of servers
	- Simply upload code and access from other lambdas or REST APIs
- Kinesis
	- Collect data and process it in real time

- Go to IAM: create role
	1. Check AWS service. Check lambda. Next
	2. Pick policy: AWSLambdaKinesisExecutionRole
		- Can see details in JSON view
		- Shards - can break down stream in multiple partitions
		- Logs - lets other services see logs
		- Resources: * - can access any Kinesis stream
	3. Role name: lambda-kinesis-role. Create.
- Go to Lambda: create function
	1. Author from scratch. Name: ride-duration-prediction-test
	2. Python 3.9
	3. Architecture: x86_64
	4. Permissions: lambda-kinesis-role
	- Can see Python code
		- Reads event, context
			- Even will be the request to the web endpoint
		- Print event as json string
	- Deploy
	- Test: Create new event
		1. Call it 'test'
		2 Note event but don't change it
		3. Save
		- Test it. Can see return, and what we sent.
	- Lambda can be whatever you set it to. 
		- def predict(features):
			- Return 10.0
		- copy prepare_features function
		- Set ride = event['ride'] in lambda_handler
			- Set ride_id = event['ride_id']
			- *Need* a way to identify rides/predictions 
		- Call prepare features, and then predict.
		- Change return to return prediction and ride_uid
	- Change test to dummy dictionary from README

- Now have to figure out how to bring model to lambda, and use event stream
	- Print event so we see how Kinesis sends data

## Kinesis integration
- Go to AWS Kinesis: Kinesis Data Stream
	1. Name: ride_events
	2. Capacity mode: Provisioned
	3. Shards: 1
		- Low throughput stream so its fine
		- Pay for each shard per hour (2cents/hour)
	4. Create. Copy name
 
- Hit Add Trigger from lambda screen
	- Choose Kinesis: ride_events
	- No need to change anthing else
- Forgot to attach role to policy we made. Go back to IAM and add it to the role.

## Testing
- Get sending data example from README
- Go to bash terminal:
	- KINESIS_STREAM_INPUT=ride_events
	- aws kinesis put-record \
    		--stream-name ${KINESIS_STREAM_INPUT} \
    		--partition-key 1 \
    		--data "Hello, this is a test."
	- Can see successful exection
		- Has ShardID, SequenceNumber
- Check Lambda logs
	- Follow to CloudWatch
	- We see errors
		- We can see there is a retry mechanism
	- We forgot to define ride_id in the code. Deploy
	- Back in CloudWatch we can see successful log
		- Can view JSON log
		- Can consume up to 100 events at once
		- Can see meta information, where event came from, eventSourceARN (there can be multiple streams)
		- Actualy data is under kinesis: data
			- Encoded in base64 to see what was sent
- Back to Lambda
	- import base64
	- Loop over event['Records'] 
		- Get record['kinesis']['data'], and decode
		- Print decoded data
	- Get JSON from CloudWatch, make it into a new test. Run the test
		- Now we can see the JSON printed and the decoded payload
	- Now we want to send an actual event.
		- Get the 4th example from the README and run it
	- Go to lambda logs
		- Can see the encoded JSON went through
	- Edit for loop
		- Set the jsonified and decoded data to ride_event
	- Change test-kinesis to the new JSON from Kinesis. Test.
		- Notice we can see the decoded data now
	- Get commented code and add it to for loop
		- Create predictions list before for loop
		- Append each prediction at the end of each loop
		- Return predictions in a dictionary as 'predictions at the end
	- There was an error; KeyError: 'ride'. Change to ride_event
		- Retest. Works now
		- Comment out print statements

## Second stream
- Want to push predictions to another stream instead
	- In lambda, turn append into its own variable and then append that to predictions
- Back to Kinesis
	- Make a new one called ride_predictions
	- 1 shard
	- Create
- Back to Lambda
	- Update prediction_event
		- Add 'model' entry
		- Add 'version' entry
		- Add prediction in entirely as its own dict
	- Add sending to kinesis
		- import boto3
		- Create client for kinesis
		- Get syntax example for put_record from boto3 docs
			- Should be using put_records but this is an example
			- Put in loop
			- Get StreamName from env
			- JSON dump prediction_event for Data
			- PartitionKey is ride_id converted to str
	- Test. Notice the error is because the role does not have the PutRecord permission
		- Back to IAM - Attach Policy
		- Create policy - Add putRecord(s) for Kinesis
		- Attach to role
		- Should work now.

## How to read from stream and verify function
- Go to 'Reading from the stream" snippet in README
	- Need Shard-id
	- TRIM_HORIZON means that everything in the stream should be returned
	- Returns ID of an iterator
- Lambdas hide all this complexity
	- Need more code without lambda
- Now can pass this to get-records to get the stream object
	- Use the echo command to get just the payload

## Getting a model in Lambda
- Copy code into local editor
- Want to use model in predict()
	- Get code to get logged model via run_id from S3
		- Copy to top of py file
	- Change predict() to use model.predict(features)
- Create test.py
	- import lambda file
	- Use event as the dummy Kinesis event
	- Send it to the lambda_handler function and print result
- Test test.py
	- Needed to install boto3
	- Need to specify RUN_ID
		- Check README for 'Running the test'. Replace RUN_ID
	- Comment out kinesis put_record in code
		- Set it to run based on env variable DRY_RUN
	- Try to run
		- 'Object of type ndarray is not JSON serializable'
	- Need to return float(pred[0]) from predict()
		- JSON cannot handle a np array

## Saving environment
- Make pipfile
	- Delete if needed
	- `pipenv install boto3 mlflow scikit-learn --python=3.9`
- Make dockerfile
	- Go to AWS ECR gallery
	- Find Lambda Python
		- 'lambda/python' 
		- FROM public.ecr.aws/lambda/python:3.9
	- Update pip, pipenv
	- Copy pipfile in
		- Install it
	- Copy in code
	- Choose entrypoint for Lambda
		- Run CMD "lambda_function.lambda_handler"
- Build dockerfile
	- Update docker run to check env variables
		- PREDICTION_STREAM_NAME
		- RUN_ID
		- TEST_RUN
- Run image
	- Use test_docker.py
		- import requests
		- Copy over event from test.py
		- Run against docker endpoint
	- Fails with NoRegion() error
		- Add it to env variables (AWS_DEFAULT_REGION)
	- Adjust docker run command to add the following as env variables
		- AWS_ACCESS_KEY_ID, 
		- AWS_SECRET_ACCESS_KEY
		- AWS_DEFAULT_REGION
	- Fails with NoCredentialsError
		- Couldn't get this to work
	- Error unpickeling model
		- Need to specify scikit-learn==1.0.2 in Pipfile and rebuild env

## Pushing to AWs
- Push image to ECR
	- Use `aws ecr create-repository --repository-name duration-model`
		- Save 'repositoryUri'
	- Set this to REMOTE_URI
		- Keep tag v1
	- Docker tag ties local name to remote name
	- Before we can push, we must authenticate with `aws ecr get-login --no-include-email`
	- Now run the bash code 'Pushing'

## Back to Lambda
- Create function > Container Image
	- Name 'ride-duration-prediction'
	- Put URI of the container image from `echo $REMOTE_IMAGE`
		- Remember Lambda Kinesis role
	- Add env variables
		- PREDICTIONS_STREAM_NAME="ride_predictions"
		- RUN_ID="86bf545d5c6c4a14b59b44c933a4c42b"
	- Add trigger Kinesis > ride_events

## Test lambda
- May need to add S3 permissions
- Run 'export KINESIS_STREAM_INPUT=ride_events`
- Send kinesis event from CLI again
	- Check logs
- Add RAM (256 MB) and timeout 15 sec
- Go back and test via Lambda test function
	- Need to test twice because of timeout
- Try again from CLI then check CloudWatch
	- See that it worked
	- Check from stream with results to check
	- Had to get SequenceNumber from latest CloudWatch log
	- Take data payload and base64 decode
