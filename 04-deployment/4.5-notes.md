# 4.5 Batch: Preparing a scoring script
- Use ride duration prediction model again though it is not ideal
	- Web service would be ideal
- But we can think of this using the actual duration and predicted duration
	- Then check the deviation between these two and save those for analytics
- Better to stick to this than to start with another use case

## Getting code
- Get random-forest.ipynb from web-service-mlflow/
	- Make it self contained py file
- Then deploying/scheduling this script with Prefect

- Copy notebook over as score.ipynb
	- Get S3 code from other web-service-mlflow/predict.py
	- Overwite mlflow connection code
- Model is actually predicting seconds, not minutes. Just ignore it for the sake of learning
	- Won't have this duration variable in practice either but it's fine
- Move categorical variable to prepare_dictionaries()
- Remove df_train and df_val, as well as target. Just leave one df
	- Set y_pred = model.predict(dicts)
- Remove training
- Now we want df with results
	- We would want a ride_id in df_results
		- If we concatenate e.g. pickup time, PULocationID and DOLocationID that should be pretty unique
		- Or we can create the same uuid
			- Import uuid and call uuid.uuid4() and set as string
	- Usually a UUID is already present so we're pretending it was
- Create list of UUIDs and set it to a column of the df
- Now we want some metadata for analytics
	- Keep pickup datetime and the pickup/dropoff location IDs
	- Set actual_duration as the duration and set predicted_duration as the result
	- Find the diff of the durations and set it
	- Add model version
- Create input_file and output_file parameters
	- We can set a URL
	- Parameterize URL and output file
- Make function for uuids
- Make function for apply_model and put the model download there
- Run and test

## Making py file
- Use `jupyter nbconvert --to script <notebook>`
- Create function run()
	- Put all the global code in
	- Add if name main to run run()
- Parameters:
	- [1] as taxi_type
	- [2] as year
	- [3] as month
	- [4] is your run id 
- 4 parameters is enough to justify argparse but we are lazy
- Add logging in apply_model()
	- When reading the data and filename/url
	- When loading the data
	- When applying the model
	- Saving the results to output file/output save location
