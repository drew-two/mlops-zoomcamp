# Three Ways to Deploy A Model

Recap: MLOps Lifecycle
1. DESIGN
	- Find requirements
	- See if ML is a decent solution
2. TRAIN
	- Experiment tracking
	- Notebook into pipeline
	- **Model artifact**
3. OPERATE
	- Deploy model
	- Monitor deployment

## Deployment
- Do we need predictions in real-time or no?
	- If not, go with batch model deployment (or offline deployment)
		- Run regularly: Model not running all the time, just apply to new data regularly
	- Real-time/online: Running all the time
		- Web service
		- Or streaming (model listening for events)

### Batch Mode
- Apply model regularly (every 10 minutes, everyday, etc)
- Usually has some DB, job with model pulls some data and applies model.
	- Running on an interval, so get data from last interval 
	- Then puts predictions in DB
	- Any service can pull from this second DB (Predictions, dashboard etc)

### Marketing
- Batch mode usually used for marketing
- Say you have someone calling a taxi.
	- Uber wants to compete, user goes to Uber. This is called **churn**
- Want to identify users who about to churn, then give some kind of incentive
- These kind of models are useful in batch mode because you don't need this data immediately
	- Maybe run this task every week
	- Ask, how often does the business need a result?
- Churn model pulls from DB, puts predictions in DB, marketing job pulls data and acts on it

### Web Service 
- 1:1 client-server relationship
	- Connection between client and server during entire time of model run
- Say we have model trying to predict ride duration
	- User uses app, which accesses model with REST API with some parameters of the user
	- Ride duration model predicts some time and then passes the data to the user
- Needs to be in real-time as user wants to know taxi time immediately

### Streaming		
- 1:N or M:N relationship
- Have Producers and Consumers (software architecture-wise)
	- Producer pushes events to event stream (buffer/queue)
	- Multiple consumers read these events and do something	
- User talks to app, the back-end is the producer and sends to stream
	- Has the user info, ID, ride start location, ride end location etc
	- Consumers take this and all predict something
		- One may tip prediction, might ask user for tip
		- Duration prediction model, to give more complicated estimate
			- May have simple one that the backend uses
			- Consumer has model to provide more accurate info during trip to user
	- Producer has no explicit connection between producer and consumer. Stateless and does not expect anything
		- Doesn't know or care about number of consumers if any

- Good use case is Content Moderation
- E.g. Youtube, user uploads video, pushes to stream
	- Consumers:
		- One consumer might look for copyright violations
		- One consumer might look for NSFW content
		- One consumer might look for violence
	- These consumers push to a steam that is read by Decision Service/Moderation Service
	- Decision sees that consumer sees why video should be removed
	- Explains why video is removed, notifies user and stops upload
- Does not have to be real-time
- Can generally add as many consumers as you want for content moderation

- Good use case: Recommender system
- E.g. Who would want to see this content? Why?
	- Push this content to that users feeds
