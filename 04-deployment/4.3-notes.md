## 4.3 Getting Models from the Registry

Recall: We made a file for prediction that makes a webservice endpoint.
	- Made dockerfile and ran container
	- Accessed by endpoint or as function
	- Tested both ways with test.py

## 
- (Optional) Use remote VM to run MLflow server
- Create S3 bucket for artifacts
	- Run `mlflow server --backend-store-uri=sqlite:///mlflow.db --default-artifact-root=s3://...`
		- Ensure artifact root is your bucket name
	- Postgresql would have been a better option but sqlite is fine (its included with Python)

## Adjusting Notebook
- Convert prepare_data(data) to prepare_dictionaries(df: pd.DataFrame)
	- Ensure only dataframes with pydantic
- Get dict_train and dict_val from prepare_dictionaries()
	- Just naming

- Note params `params = dict(max_depth=20, n_estimators=100, min_samples_leaf=10, random_state=0)`
	- Log these
- Create dicvect and RandomForestRegresso
- Fit and transform dict_train to X_train and same for validation
- Predict, get RMSE
- Log all these, and log dictvect to bin

- Run, then check MLflow UI > green-taxi-duration > latest run > Artifacts
	- Note run id and copy it as a variable
- First, copy Pipfiles, predict.py, test.py from ../web-service/

## Adjusting predict.py
- import mlflow
- Set `RUN_ID`, set `logged_model = f'runs:/{RUN_ID}/model'`
- Import mlflow client and open it to localhost:5000
	- Download the dictvect from the same run id
- Useful to return model version to return payload

- Run `pipenv install mlflow` in the directory  

- Will fail if you do not set:
	- tracking_uri
	- experiment
- Now you can test with test.py after running predict.py

## Making MLflow less ugly
- Instead of having a dictvect and model we can make a pipeline
- Can log the pipeline entirely in the registry
- Create pipeline using make_pipeline from sklearn.pipeline
	- Add dictvect and Random Forest functions
	- Run pipeline.fit
	- No longer need to pickle dictvect

- Go back to predict.py
	- Set run id to new run with pipeline
	- Only need tracking uri, run id, logged_model, and to pull model
	- In predict(), only need model.predict(features)

- logged_model path can also be:
	- `models:/<model_name>/stage1`

- What if tracking uri changes or goes down?
	- If there's no tracking server then predict.py cannot work
	- Note we can point to s3 location directly
		- s3://<bucket_name>/<exp_id>/<run_id>/model
- Test with test.py - it works
	- Removes dependency on tracking server just to get static model
- Better way to do this - set RUN_ID as env variable

