# Deployment of Prefect Flow

## Storage
- Referring to remote VM
- Flows have to be stored somewhere
	- When you run a script, it sends an API call to create a flow and receives the flow info back
		- This flow has to be created and saved by Prefect
	- 'prefect storage ls' - note that locally there is no storage locally
	- Could store in Azure, GCP, S3, NAS, local or temp
		- Storing locally for this example
		- ~/.prefect
		- Set as default

## Deployment Specification
- Need to import DeploymentSpec and scheduling
- When Prefect deploys flow, you have option to deploy locally
	- In Docker, spins up new container and runs flow inside it
	- In Kube, spins up new pod and runs flow there 
- Importing SubprocessFlowRunner - used to run Flow as subprocess of Python script, no container/pod
- Need to define DeploymentSpec(
	- flow = main
	- name = "model_training"
	- schedule = IntervalSchedule(interval=timedelta(minutes=5))
	- tags = [ml]'
	-) 
	- Tags can be used to find runs, like choosing Staging or Production
	- Set schedule to be quick, normally once a week or something
- Run 'prefect deployment create <python file> to run
- Error: 
	- ... failed validation! You have configured local storage but 
		this deployment is using a Universal flow runner which requires remote storage.
		Failed to create 1 out of 1 deployments.
	- Basically saying there's a chance this could run on Docker or Kube, they won't be able to pull from the local storage (host machines storage)
	- To specify flow runner only on local:
		- flow_runner=SubprocessFlowRunner(),
		- in DeploymentSpec
- Go to Prefect Orion UI
	- Can see many future runs because of the schedule
		- Note: Prefect is not providing any compute
- Work Queues:
	- In Prefect Orion Work Queue, can specify on local, Docker etc
	- Work Queues can limit what work they can accept, e.g. only flows with "docker" tag or "ml" tag
		- LIMITS amount of work
	- Can create in UI, then run prefect work-queue preview <queue id>
		- And see all the scheduled times, names, run IDs and deployment IDs
	- Queues have Agents, which check for work every 5 secs, check how to do, get flow from storage, and run work
	- Can run with:	 prefect agent start <queue id>
		- All compute happens on machine where "prefect agent start" is ran
		- Will see updates in cloud for these
	- Can filter Flow runs to state "Completed" or "Running" to see these 

- 3 things needed for flow run:
	1. Storage for flows - can be local, cloud, NAS etc
	2. Deployment - can be on local, docker, kube etc
	3. Work Queue - can have filter what it can pick. Gives agent
	4. Run agent locally or docker host

- To integrate with MLflow
	- Could write a flow that writes every week for model training
		- Compares model with model in production
		- If better, promote to production
	- This is more of a real example of Prefect in industry

